# CSE110A: Fundamentals of Compiler Design

## Homework 1: Lexical Analysis and Scanners

## Preliminaries

1. This assignment requires `python3`. This will be available on the provided docker, 
   but you can also develop the assignment locally. Developing in the docker will ensure
   that it runs correctly on Gradescope.  You can check the code output on Gradescope 
   based on the auto-grading tests for each part of HW1 (there are more instructions 
   at the end of the document).    

2. The autograded programming portion of this homework contains 4 parts. Each part is 
   worth equal points.

3. If you need help, please visit office or mentoring hours. You can also ask questions 
   on Piazza.  You are allowed to ask your classmates questions about frameworks and 
   languages (e.g. Docker and Python). You are NOT allowed to discuss technical homework 
   solution details with your classmates.

4. Your submission will consist of the code that you write plus answers to the reflection 
   questions that you will write in plain ASCII.  These question will be found in the 
  `explanations.txt` file.  But your answers after the line that says: 
   "Type your answer here".

6. Keep in mind that the programming tests that we provide you may not be the only used 
   by Gradescope to autograde the assignment. Please think about writing additional tests 
   to more thoroughly assess the correctness of your solution!

7. You will be submitting the following files: `NaiveScanner.py`, `EMScanner.py`, `tokens.py`, 
   `SOSScanner.py`, and `NGScanner.py` for the programming part of the assignment on Gradescope.
   The autograded assignments will be worth 60 points.
8. You will be answering and submitting the questions found in the "explanation.txt" file 
   from the repository root in plain ASCII.

9. Each time you want to assess your grade you will downloaded a zipped version of all your
   files, and submit it to Gradescope.  Do this as many times as you want, to observe your 
   progress.  There is no penalty for multiple submissions.

10. WARNING: When you zip the files include any files called doNotChangeMe.<anything>
    make sure you do not edit this file.

## Part 1: Extending the Naive Scanner

In this part of the assignment, you will extend the naive scanner presented in class. This 
should be a good exercise to get you started (or refreshed) in Python.

As written, the naive scanner can match the following tokens:

- **ID:** an id that consists only of a sequence of lowercase letters
- **NUM:** non-negative integers of arbitrary length
- three single character operators:
  - **ASSIGN:** "="
  - **ADD:** "+"
  - **MULT:** "*"

Additionally, the naive scanner ignores whitespace.

### Extensions

You will be providing several extensions to the naive scanner. You **cannot** use regular expressions 
and must simply use the *StringStream* object and character comparisons. You cannot change the 
public interface to the scanner, i.e. you cannot change the function names, arguments, or what 
they return. You are free to add new functions if you need them, or modify the code in the existing 
functions as much as needed.

You must provide the following extensions:

- a new token called **SEMI** that matches ";"
- the **ID** token will be able to contain numbers as long as they are not the first character of the **ID**
- the **NUM** token will be able to have an optional decimal point ".", but must not contain more than 
  one decimal point. If there is a decimal point then there must be a number after the decimal point.
- a new token called **INCR** which matches "++"

### Skeleton

You will be given the naive scanner code from class. The code takes a file as an argument to perform 
lexical analysis on. You can additionally provide a -v flag to print out the tokens.

If the scanner has tokenized the entire string, the `token()` function should return *None*. If it 
runs into an issue where it cannot tokenize, it should throw a `ScannerException` (as seen in the 
skeleton code).

In the root directory, you are given a directory with test input files that the skeleton naive scanner 
can tokenize. You should think about writing more test cases for your implementation, as you are 
extending it past what these test inputs check for.

The test files are located in the directory: `../tests`, where each file has a number suffix 
corresponding to how many random (non-ignored) tokens it contains.

For example, to run the test file with 10 tokens in it, you would run:

```bash
python3 NaiveScanner.py ../tests/test10.txt
```

To run the test input while printing the output, you would run:

```bash
python3 NaiveScanner.py -v ../tests/test10.txt
```

You should see the lexemes printed out.

### What to Submit

For the coding part of the question, you will submit a completed skeleton file (i.e. `NaiveScanner.py`).

The reflection questions for this part will include some descriptions and experimental timings. For 
each new or modified token, write a few sentences about how you implemented it. Without printing the 
lexemes, time your scanner on the 10, 100, 1000 and 10000 test cases and report the timings. 
Please run this experiment on your local machine and comment on the scaling.

Your grade will be based on 4 criteria (address these in your explanations):

- Correctness: do your scanner extensions accurately implement the required behavior?
- Conceptual: does your implementation design conceptually match the design of a naive scanner
- Experiments: did you run the required experiments and report the timings?
- Explanation: does your report explain your implementation design and comment on the performance?

In addition to the token-numbered test files, there are a few more test files provided to you, 
referred to as sanity checks. These test files are present under `tests` You can run these tests 
as input to your NaiveScanner, as shown below.

```bash
python3 NaiveScanner.py ../tests/part1.txt
```

There are autograding tests in Gradescope. The purpose of these checks is to help you discover 
any "obvious" errors in your code. If a test fails, it indicates the presence of a bug in your 
code that requires resolution. You can push your code to Gradescope as often as needed before the deadline.

## Part 2: Regular Expressions for Token Definitions

In this part of the homework, you will write regular expressions for token definitions. 
The aim is to provide a scanner for a simple programming language.

The tokens you need to implement are:

- **ID:** an ID consisting of lower case letters, upper case letters, or numbers. It cannot start with a number.
- **NUM:** a non-negative integer number. It can optionally contain a decimal point, but if it contains a decimal point, it must contain a number after the decimal point (e.g. "56." is not allowed). A number before the decimal point is not required.
- **HNUM:** a hexidecimal number. Like in C, it should start with a 0x followed by digits, which can include a-f. The characters should be case insensitive.
- **INCR:** "++" (as in part 1)
- **PLUS:** "+"
- **MULT:** "*"
- **SEMI:** ";"
- **LPAREN:** "("
- **RPAREN:** ")"
- **LBRACE:** "{"
- **RBRACE:** "}"
- **ASSIGN:** "="

There is a special token called **IGNORE**. You should ignore whitespace and newlines. You must 
additionally provide a token for each of the keywords, if they match their corresponding text:

- **IF:** "if"
- **ELSE:** "else"
- **WHILE:** "while"
- **INT:** "int"
- **FLOAT:** "float"

### Skeleton

You should implement these tokens in the following file: `part2/tokens.py`. I have provided a 
start for some of them. Please continue in the provided style. Each token definition is a tuple 
with a token name, a regular expression definition and a token action.

Please implement your keywords as a token action on the ID token. We will discuss token actions 
on the last day of the module. You can get started by ignoring the keywords for now.

We have provided the exact match (EM) scanner in the `EMScanner.py` file. This scanner imports 
the tokens and creates an exact match scanner with them. You can use this to test your token 
definitions. You can try running them on the test files, as in part 1. However, please remember 
that these test files are not complete, so you should write your own test files.

The EM scanner is slow and you may not be able to run it on large test files. That is, don't try 
to run it files larger than 100 tokens.

### What to Submit

You will submit a completed `tokens.py` file with the updated tokens.

In the questions found in (i.e. in `Gradescope`), please write about your RE definitions and 
some timing information for running the EM scanner.

Similar to part 1, there are checks that you can use to check for any "obvious" errors. 
The test files are present inside `tests`. In particular, for this part of the homework, 
you should be able to successfully tokenize `part2.txt`.

You can run this file with the EMScanner directly, but it will also be run as part of 
the Gradescope submission and you can see the results on it. In the event that any of 
these checks fail, it indicates that there is a bug or error present within your program 
that you should fix. Recall that you can submit as many times as you'd like before the 
submission deadline.

## Part 3: Implementing a Start of String Scanner

In this part of the homework, you will implement a start of string (SOS) scanner (as 
discussed in class). You should use code of the EM scanner given in part2 as a guiding 
example. However, you should use `re.match` rather than `re.fullmatch`. This should 
allow you to check each token regular expression once per call to `token()`, rather 
than for each substring (as done in the EM scanner).

### Skeleton

Implement your SOS scanner in: `part3/SOSScanner.py`. Once implemented, you will be 
able to run the SOS scanner similar to the EM scanner in part 2, using your same tokens.

### What to Submit

You will submit a completed `SOSScanner.py` file. Please make sure your updated tokens 
are pushed to the Gradescope submission.

The questions for this part will include a few sentences about how your SOS implementation 
improves on the EM scanner. Without printing the lexemes, time your scanner on the 
10, 100, 1000 and 10000 test cases and report the timings. Which scanner is more 
efficient and by how much?

Your grade will be based on 4 criteria (address these in your explanations):

- Correctness: do your scanner extensions accurately implement the required behavior?
- Conceptual: does your implementation design conceptually match the design of a SOS scanner
- Experiments: did you run the required experiments and report the timings?
- Explanation: does your report explain your implementation design and comment on the performance?

You can additionally use the check for part 2 to check your SOS Scanner (and you can also 
see the autograding results on Gradescope).

## Part 4: Implementing a Named Group Scanner

This part is the same as part 3, except your scanner will build one giant RE out of the 
individual token REs. You will use Python's named groups to find the matched strings. 
That is, you will create a Named Group (NG) scanner.

This implementation should improve on the SOS scanner as it should only match on one 
RE per call to `token()`.

### Skeleton

Please implement your NG Scanner in: `part4/NGScanner.py`.

### What to Submit

You will submit a completed `NGScanner.py` file. Please make sure your updated tokens.py 
is included in the zip file you push to Gradescope.

The questions for this part will include some descriptions and experimental timings. 
Write a few sentences about how your NG implementation improves on the SOS scanner. 
Without printing the lexemes, time your scanner on the 10, 100, 1000 and 10000 test 
cases and report the timings. Compare your NG scanner times to your SOS scanner times.

Your grade will be based on 4 criteria (address these in your explanations):

- Correctness: do your scanner extensions accurately implement the required behavior?
- Conceptual: does your implementation design conceptually match the design of a NG scanner
- Experiments: did you run the required experiments and report the timings?
- Explanation: does your report explain your implementation design and comment on the performance?

You can additionally use the check for part 2 to check your NG Scanner (and you can also 
see the autograding results on Gradescope).

## Challenge

This is NOT for extra credit, but there is a small optimization you can do that should increase 
the speed of your scanners significantly. See if you can find it!

## Submitting to Gradescope

You should download your GitHub Classroom repository as a zip file and submit it directly 
on Gradescope. There is a single submission for this homework.

As mentioned in the introduction, signing up for the GitHub Classroom assignment will 
automatically create a repository for you under the `ucsc-cse110a` organization. This 
repository is for you to store the changes to your assignment, and help you have the 
complete template for the assignment. Do not rename the homework files, as this will 
cause the autograding scripts to fail. Do not modify anything inside the `.github/` 
folder. The only files that should need editing for this assignment are 
`part1/NaiveScanner.py`, `part2/tokens.py`, `part2/EMScanner.py`, `part3/SOSScanner.py`, 
and `part4/NGScanner.py`. Any others may be overwritten by the autograding scripts.

Your explanations should be written in plain ASCII in the repository root in `explanations.txt`.

The autograding is set on Gradescope, and when you submit all the required files for each 
part, it will automatically run all the tests.

The results of the autograding pass/failures can be seen in Gradescope. We reserve the right 
to run more tests than what are providing you.

As a reminder, it is advisable not to wait until the last minute to submit. Submit as often
as you want.  There is no penalty for many submissions, in fact we encourage it.

Remember in addition to pushing your code solution to Gradescope, also answer the questions 
in the `explanation.md` and push it into your repo as well, prior to downloading your zip
file.

